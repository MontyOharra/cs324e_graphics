Initially, I thought this project was just about applying different convolutions as single step operations (I probably should've read the description before starting).

So, this is all for the convolution filter.

I wanted to implement functionality for both larger convolution sizes, as well as more advanced edge convolution calculation. In order to achieve this versatility, I implemented two different custom "datatypes" (or at least that's what I think of them as) in the CoordFindingFunction interface and the Coord class.
The CoordFindingFunction is simply an interface that allows users to pass a function of 4 int parameters as a type, and the Coord class stores two values, x, and y, in order to make the code more readable when access coordinates.

There is a comment at the top saying that the code of these two custom types was generated by ChatGPT.

The two functions, getCoordsNoEdge, and getCoordsReflectEdge, take in the size of the image, as well as the location of the current pixel and use this in order to determine what the target pixel coordinate should be when calculating a convolution that goes past the edges of the image. These functions are all very similar, and allow for user defined functions to be created. In order to create one of these functions, the CoordFindingFunction<Coord> return type must be used, along with 4 int types as parameters. The way the functions should be setup is to have two if else if clauses that determine the behavior of how a coordinate is returned if it goes out of bounds.

These functions are used by the getConvWindowCoords function, which has the purpose of returning the full window of coordinates to be applied on a convolution. It loops through the neighborhood of the given pixel value, and applies the coordFunction parameter in order to return the coordinates of the targeted pixel during a convolution.
This function will return null if it is given a value of -1 on either the x or y coordinate of a Coord object, allowing for edge calculation functions that ignore applying convolutions on some edges. 

applyConvolution then takes an image, a kernel of any size, and and a coordinate calculating function. Looping through each pixel in the image, tt runs the getConvWindowCoords function in order to get the array of pixel coordinates that the convolution should be applied to when given the current pixel as the center of the convolution window. If the getConvWindowCoords function returned null, then the convolution is ignored and the pixel is assigned its original value. Otherwise, the convolution window is looped through on both the x and the y, and the convolution weight for that specific pixel in the neighborhood is applied to the current color values. Then new color value is then determined from this and pushed to the new image.


For the grayscale filter, a similar process is used, but instead of applying some convolution, the red, blue, and green color channels are simply averaged and this average is applied to the red blue and green channels of the new image's pixel.

For the contrast filter, there is a threshold for whether a pixel's color channel should be increased or decreased, as well as the value that it should be increased or decreased by.

I was not able to get a proper implementation for the edge detection working. 